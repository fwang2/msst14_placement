\section{Related Work}
\label{sec:relatedwork}

In this section, we survey several existing works related to our paper. We classify these existing works into two categories. The first category consists of existing works on data placement algorithms for distributed storage systems, while the second consists of those on hybrid storage systems that aim to leverage SSD drives to improve data access performance.

As large-scale distributed storage systems have been extensively used in the HPC area, the problem of distributing several petabytes of data among hundreds or thousands of storage devices becomes more and more critical. To address this problem, many data placement algorithms have been proposed. For instance, Distributed Hash Tables (DHTs) have been used to place and locate data objects in P2P systems \cite{Stoica2001, Ratnasamy2001, Cai2004}. Another replica placement scheme called chain placement was also proposed and applied to some P2P and LAN storage systems \cite{Rowstron2001, Lee1996, MacCormick2004}. Honicky and Miller presented a family of algorithms named RUSH \cite{honicky04} that utilizes a mapping function to evenly map replicated
objects to a scalable collection of storage devices, so that it can support efficient additions and removals of weighted devices.

To address the reliability and replication issues of the RUSH algorithm, Weil et al. proposed a scalable pseudo-random data distribution algorithm named CRUSH \cite{Weil2006}. Besides optimally distributing data to available resources and efficiently reorganizing data after adding or removing storage devices, CRUSH exploits flexible constraints on replica placement to maximize data safety in the case of hardware failures. Specifically, CRUSH allows the administrator to assign different weights to storage devices so that the administrator can control the relative share of data each device is responsible for. However, the device weights used in the CRUSH algorithm only reflect the capacities of storage devices, therefore, the CRUSH algorithm may not be effective anymore for hybrid storage systems consisting of both SSD and HDD devices, as these two kinds of storage devices have totally different performance characteristics.

Recently, efforts have been made to combine SSD and HDD drives together to construct hybrid storage systems. In such systems, SSDs are either used for caching purposes, or used as more independent storage devices. For example, Srinivasan et al. designed a block-level cache named Flashcache \cite{Flashcache} between DRAM and hard disks using SSD devices. Zhang et al.
proposed iTransformer \cite{Zhang2012} which exploits a small SSD to schedule requests for the data on disks so that high disk efficiency can be achieved. SieveStore \cite{Pritchett2010} adopts a selective caching approach in which the accesses of each block are tracked and the most popular block is cached in SSD device. In the second approach, SSDs are more independently used. Chen et al. designed and implemented a high performance hybrid storage system named Hystor \cite{Chen2011}, which identifies data blocks that either can result in long latencies or are semantically critical on hard disks, and store them in SSDs for future accesses. In order to prolong the service life of SSDs devices, Ren et al. proposed I\_CASH \cite{Yang2011} to reduce random write traffic to SSDs. Specifically, I\_CASH is an approach that exploits the spacial locality of data accesses, and only store those seldom-changed data blocks on SSDs. Finally, ComboDrive \cite{Payer2009} concatenates SSD and HDD into one address space via a hardware-based solution, so that certain data on HDD can be moved into the faster SSD space.

There are two main differences between existing works on hybrid storage systems and our approach: first, most existing works on hybrid storage systems only consider how to improve the utilization of SSD drives, but they have ignored the reliability and replication issues in HPC environments; second, existing works have not considered the dynamic nature of workflows, a nature that makes continuous training and learning necessary. In our approach, we fully consider these issues, and our method provides up-to-date predictions on popular data blocks, so that we can store critical data on SSDs well in advance.
