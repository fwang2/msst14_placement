\section{Introduction}
\label{intro}

With the explosive increase in the amount of data being generated by various HPC applications, developing resilient and high-performance storage solutions becomes even more of a challenge \cite{lofstead2010managing,DBLP:conf/mss/ZhuWLHLM13}. Therefore, it is crucial to come up with effective data placement mechanisms in a heterogamous environment based on the nature of workloads, as well as the properties of the underlying hardware such as their network topology and bandwidth. On the other hand, both the workloads and the underlying hardware evolve over time, so it requires algorithms have to be designed to take these changes into their consideration.

One particular trend that has emerged in recent years is the use of SSD drives in storage solutions to provide \emph{premium} services, as the read and write speed for SSD drives are typically much faster compared to hard drives~\cite{park2009performance}. On the other hand, SSD drives have different failure patterns where repeated read and write operations of the same address blocks will cause the drives to fail~\cite{chen2009understanding}. Furthermore, SSD drives are also limited in capacity, meaning that it is not yet practical to use them to completely replace conventional hard disks. Therefore, how to effectively integrate SSD drives into the design of storage systems for HPC environments becomes a critical yet challenging task.

Existing algorithms to integrate SSD drives can already be found in the literature~\cite{DBLP:conf/mss/WangHHZLH13, DBLP:conf/mss/LeeKKLPM13}. These methods, while effective, are largely based on heuristic algorithms that are either developed in isolation with the runtime workload, or are based on static assumptions on the workload patterns, making them unsuitable when the underlying workloads and demands change over time.

To address such drawbacks, we present a holistic approach where we aim to develop a framework that adaptively classifies the popularity of data objects, adjusts their placement among storage tiers by moving them between slower HDDs and faster SSDs, and fulfills the needs of users with regard to their I/O operation requirements. Formally, our developed algorithm makes the following assumptions. First, we assume that the storage hardware consists of both slower HDD drives and faster SSD drives. Second, the user may have their own rules for the placement of data, such as, a particular data object must be stored on HDDs with three copies available for a certain period of time, among others. Therefore, such rules must be properly fulfilled during runtime.

Based on these assumptions, our proposed method makes two contributions: first, it proposes a Markov-chain based classification model to predict whether data objects will be accessed frequently in the future based on their historical access records; second, it develops an integrated data placement engine that is based on linear programming for fulfilling the requirements of throughput and reliability from the users.  We next describe these two contributions separately.

In the first contribution, we classify data objects based on their access patterns, including both their access frequencies and the particular workload that accessed them, so that we can determine those objects that are most likely to be accessed frequently in the future. Our method is based on training a Markov chain model, and once such predictions are made, we move
those frequently accessed objects to SSD drives under the constraint that the moving cost does not exceed the predicted savings.

In the second contribution, we consider the challenge on fulfilling the user policies on data placements, such as their preferences on where to place the objects. To this end, we develop a data placement engine that takes user policies and the performance variation between storage devices, such as the bandwidth and delays between two HDDs or SSDs, as input, and generates a satisfying solution, if any, as the output. The theoretical foundation of this engine stems from linear programming, where we allow numerical methods to be adopted to find solutions. For example, if the user specifies that no two
copies of a data object should be on the same rack, such a requirement will be formulated as a constrain in the linear programming model and be guaranteed in the solution.

%The key contributions of the developed algorithm are listed as follows: first, we present an access-frequency sensitive algorithm for extending the CRUSH algorithm. The core methodology involves modeling the different storage services based on their access latency parameters, and try to minimize the overall access time through an optimization problem.
%
%Second, we observe that given that users' request patterns change over time, it is time-consuming to re-solve the problem every time the user input changes. Therefore, we develop a learning algorithm to classify the input patterns, and generate the output parameters automatically after the training phase. This approach allows the extension of CRUSH to handle different application requirements highly efficiently.


The rest of this paper is organized as follows. We describe the related work in Section~\ref{sec:relatedwork}. The design is described in Section~\ref{sec:design}. The performance evaluation is given in Section~\ref{sec:evaluation}. We provide conclusions in Section~\ref{sec:conclusion}. %We survey related work in Section~\ref{sec:related}.
















